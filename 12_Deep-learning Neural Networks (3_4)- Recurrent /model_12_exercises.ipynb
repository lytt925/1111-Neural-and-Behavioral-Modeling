{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural & Behavioral Modeling - Week 12 (Exercises)\n",
    "by 李彥廷 (b08207008@ntu.edu.tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True \n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import *\n",
    "from IPython.display import *\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.0\n",
      "No GPU\n"
     ]
    }
   ],
   "source": [
    "# Check GPU status:\n",
    "print('PyTorch version:',t.__version__)\n",
    "use_cuda=t.cuda.is_available()\n",
    "if(use_cuda):\n",
    "    for i in range(t.cuda.device_count()):\n",
    "        print('Device ',i,':',t.cuda.get_device_name(i))\n",
    "    print('Current: Device ',t.cuda.current_device())\n",
    "    t.backends.cudnn.benchmark = True \n",
    "    device = t.device(\"cuda\")\n",
    "else:\n",
    "    device = t.device(\"cpu\")\n",
    "    print('No GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNN on images (4 points)\n",
    "Note that the RNN-based MNIST classifier in 2.1.1 of model_12_examples.ipynb did not apply a softmax as the response function of its last layer.\n",
    "\n",
    "Does adding a softmax make any difference in model prediciton performance? Why or why not?\n",
    "\n",
    "Refs:\n",
    "\n",
    "[1] https://pytorch.org/docs/stable/nn.html \n",
    "\n",
    "[2] https://pytorch.org/docs/stable/nn.functional.html \n",
    "\n",
    "[3] https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Please run your computational experiments and write your observations accordingly.\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5],std=[0.5])])\n",
    "\n",
    "train_set = datasets.MNIST(root = \"./data\",\n",
    "                               transform = transform,\n",
    "                               train = True,\n",
    "                               download = True)\n",
    "\n",
    "train_data = t.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "test_set = datasets.MNIST(root = \"./data\",\n",
    "                              transform = transform,\n",
    "                              train = False)\n",
    "\n",
    "test_data = t.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting functions:\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_acc(output, label):\n",
    "    total = output.shape[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    return num_correct / total\n",
    "\n",
    "\n",
    "def train(device, net, train_data, valid_data, num_epochs, optimizer, criterion):\n",
    "    if t.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "    prev_time = datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        net = net.train()\n",
    "        #for im, label in train_data:\n",
    "        for im, label in train_data:\n",
    "            im=im.view(-1,im.shape[2],im.shape[3])\n",
    "            im = im.to(device)  # (bs, h, w)\n",
    "            label = label.to(device)\n",
    "            # forward\n",
    "            output = net(im)\n",
    "            loss = criterion(output, label)\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.data.item()\n",
    "            train_acc += get_acc(output, label)\n",
    "\n",
    "        cur_time = datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_loss = 0\n",
    "            valid_acc = 0\n",
    "            net = net.eval()\n",
    "            for im, label in valid_data:\n",
    "                with t.no_grad():\n",
    "                    im=im.view(-1,im.shape[2],im.shape[3])\n",
    "                    im = im.to(device)\n",
    "                    label = label.to(device)\n",
    "                output = net(im)\n",
    "                loss = criterion(output, label)\n",
    "                valid_loss += loss.data.item()\n",
    "                valid_acc += get_acc(output, label)\n",
    "            epoch_str = (\n",
    "                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n",
    "                % (epoch, train_loss / len(train_data),\n",
    "                   train_acc / len(train_data), valid_loss / len(valid_data),\n",
    "                   valid_acc / len(valid_data)))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %\n",
    "                         (epoch, train_loss / len(train_data),\n",
    "                          train_acc / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoe model in 2.1.1 of 13_examples.ipynb:\n",
    "\n",
    "class RNN_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN_1, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size = 28,\n",
    "                                hidden_size = 128,\n",
    "                                num_layers = 1,\n",
    "                                batch_first = True)\n",
    "        \n",
    "        self.classifier = nn.Linear(128,10)\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output,_ = self.rnn(input, None)\n",
    "        output = self.classifier(output[:,-1,:])\n",
    "        output = self.Softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoe model in 2.1.1 of 13_examples.ipynb:\n",
    "\n",
    "class RNN_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN_2, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size = 28,\n",
    "                                hidden_size = 128,\n",
    "                                num_layers = 1,\n",
    "                                batch_first = True)\n",
    "        \n",
    "        self.classifier = nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output,_ = self.rnn(input, None)\n",
    "        output = self.classifier(output[:,-1,:])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of RNN with Softmax\n",
      "Epoch 0. Train Loss: 1.804475, Train Acc: 0.672058, Valid Loss: 1.670232, Valid Acc: 0.796477, Time 00:00:04\n",
      "Epoch 1. Train Loss: 1.675504, Train Acc: 0.788380, Valid Loss: 1.630095, Valid Acc: 0.833101, Time 00:00:04\n",
      "Epoch 2. Train Loss: 1.650786, Train Acc: 0.811650, Valid Loss: 1.628164, Valid Acc: 0.833798, Time 00:00:04\n",
      "Epoch 3. Train Loss: 1.631496, Train Acc: 0.831573, Valid Loss: 1.586569, Valid Acc: 0.877289, Time 00:00:04\n",
      "Epoch 4. Train Loss: 1.581084, Train Acc: 0.882363, Valid Loss: 1.593996, Valid Acc: 0.870123, Time 00:00:04\n",
      "Epoch 5. Train Loss: 1.567873, Train Acc: 0.895289, Valid Loss: 1.559551, Valid Acc: 0.902468, Time 00:00:04\n",
      "Epoch 6. Train Loss: 1.562428, Train Acc: 0.899820, Valid Loss: 1.547130, Valid Acc: 0.915207, Time 00:00:04\n",
      "Epoch 7. Train Loss: 1.554359, Train Acc: 0.907316, Valid Loss: 1.576294, Valid Acc: 0.886545, Time 00:00:04\n",
      "Epoch 8. Train Loss: 1.547915, Train Acc: 0.914246, Valid Loss: 1.561148, Valid Acc: 0.901672, Time 00:00:04\n",
      "Epoch 9. Train Loss: 1.558420, Train Acc: 0.903385, Valid Loss: 1.545687, Valid Acc: 0.917098, Time 00:00:04\n"
     ]
    }
   ],
   "source": [
    "srn = RNN_1()\n",
    "optimizer = optim.Adam(srn.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print('Result of RNN with Softmax')\n",
    "train(device, srn, train_data, test_data, 10, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of RNN without Softmax\n",
      "Epoch 0. Train Loss: 0.805994, Train Acc: 0.729028, Valid Loss: 0.436356, Valid Acc: 0.871218, Time 00:00:04\n",
      "Epoch 1. Train Loss: 0.353045, Train Acc: 0.896938, Valid Loss: 0.290964, Valid Acc: 0.916700, Time 00:00:04\n",
      "Epoch 2. Train Loss: 0.242571, Train Acc: 0.931187, Valid Loss: 0.200770, Valid Acc: 0.943969, Time 00:00:04\n",
      "Epoch 3. Train Loss: 0.202838, Train Acc: 0.941831, Valid Loss: 0.155826, Valid Acc: 0.957305, Time 00:00:04\n",
      "Epoch 4. Train Loss: 0.178245, Train Acc: 0.949910, Valid Loss: 0.179783, Valid Acc: 0.952030, Time 00:00:04\n",
      "Epoch 5. Train Loss: 0.165665, Train Acc: 0.952775, Valid Loss: 0.148497, Valid Acc: 0.958997, Time 00:00:04\n",
      "Epoch 6. Train Loss: 0.153037, Train Acc: 0.956773, Valid Loss: 0.149998, Valid Acc: 0.959992, Time 00:00:04\n",
      "Epoch 7. Train Loss: 0.140425, Train Acc: 0.960854, Valid Loss: 0.158948, Valid Acc: 0.956608, Time 00:00:04\n",
      "Epoch 8. Train Loss: 0.130926, Train Acc: 0.963386, Valid Loss: 0.127636, Valid Acc: 0.964670, Time 00:00:04\n",
      "Epoch 9. Train Loss: 0.128889, Train Acc: 0.963103, Valid Loss: 0.146067, Valid Acc: 0.960987, Time 00:00:04\n"
     ]
    }
   ],
   "source": [
    "srn = RNN_2()\n",
    "optimizer = optim.Adam(srn.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print('Result of RNN without Softmax')\n",
    "train(device, srn, train_data, test_data, 10, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從以上的結果可以看到正確率有稍微變小，且Loss在沒加softmax的RNN上面比較小，這可能是因為若將output先經過softmax轉換之後所得的數值都會變小，使得之後在算crossentropyloss的時候都變大了，使得他的表現在訓練過程可能較為不穩定，導致表現較差。以下展示了在一個假設的資料下，有經過和沒經過softmax轉換後的loss結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output with softmax tensor([0.2418, 0.6572, 0.0120, 0.0889])\n",
      "loss with softmax tensor(1.0120)\n",
      "\n",
      "output without softmax tensor([ 2.,  3., -1.,  1.])\n",
      "loss without softmax tensor(0.4197)\n"
     ]
    }
   ],
   "source": [
    "im = t.FloatTensor([2,3,-1,1])\n",
    "label = t.tensor(1)\n",
    "sm = nn.Softmax(dim=0) \n",
    "output1 = sm(im)\n",
    "output2 = im\n",
    "cel = nn.CrossEntropyLoss()\n",
    "print('output with softmax', output1)\n",
    "print('loss with softmax' ,cel(output1, label))\n",
    "print()\n",
    "print('output without softmax',im)\n",
    "print('loss without softmax' ,cel(output2, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN v.s. CNN (4 points)\n",
    "Compare the results of 2.2.1 (RNN) & 2.3.1 (CNN) in model_12_examples.ipynb.\n",
    "\n",
    "The 1D-CNN seems to predict the time series better than the RNN. Why?\n",
    "\n",
    "You may verify your hypotheses by computational experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.1 (RNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.1 (CNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please run your computational experiments and write your observations accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
